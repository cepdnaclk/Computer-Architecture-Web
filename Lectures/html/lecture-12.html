<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lecture 12: Pipelined Processors - Lectures on Computer Architecture</title>
    <link rel="stylesheet" href="../../assets/css/style.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-c.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-asm6502.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
</head>
<body>
    <header class="lecture-header">
        <div class="container">
            <a href="../../index.html" class="back-link">
                <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <line x1="19" y1="12" x2="5" y2="12"></line>
                    <polyline points="12 19 5 12 12 5"></polyline>
                </svg>
                Back to All Lectures
            </a>
            <h1 class="lecture-title">Lecture 12: Pipelined Processors</h1>
            <p class="lecture-meta">Lectures on Computer Architecture</p>
        </div>
    </header>

    <main class="lecture-content-area container">
        <div class="content-body">
            <!-- Video Section -->
            <div class="video-container">
                <div class="video-thumbnail">
                    <a href="https://www.youtube.com/watch?v=l3GqbXXB2QA" target="_blank" class="video-play-overlay">
                        <img src="https://img.youtube.com/vi/l3GqbXXB2QA/maxresdefault.jpg" 
                             alt="Lecture 12 Video Thumbnail"
                             onerror="this.src='https://img.youtube.com/vi/l3GqbXXB2QA/hqdefault.jpg'">
                        <div class="play-button">
                            <svg width="68" height="48" viewBox="0 0 68 48" fill="none">
                                <path d="M66.52 7.74c-.78-2.93-2.49-5.41-5.42-6.19C55.79.13 34 0 34 0S12.21.13 6.9 1.55c-2.93.78-4.63 3.26-5.42 6.19C.06 13.05 0 24 0 24s.06 10.95 1.48 16.26c.78 2.93 2.49 5.41 5.42 6.19C12.21 47.87 34 48 34 48s21.79-.13 27.1-1.55c2.93-.78 4.64-3.26 5.42-6.19C67.94 34.95 68 24 68 24s-.06-10.95-1.48-16.26z" fill="red"/>
                                <path d="M45 24L27 14v20" fill="white"/>
                            </svg>
                        </div>
                    </a>
                </div>
                <div class="video-info">
                    <p class="video-notice">
                        <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
                            <circle cx="12" cy="12" r="10"></circle>
                            <line x1="12" y1="16" x2="12" y2="12"></line>
                            <line x1="12" y1="8" x2="12.01" y2="8"></line>
                        </svg>
                        Click the thumbnail above to watch the video lecture on YouTube
                    </p>
                </div>
            </div>

            
<em>By Dr. Isuru Nawinne</em>

<h2>12.1 Introduction</h2>

<p>This lecture introduces pipelining as the primary performance enhancement technique in modern processor design, transforming the inefficient single-cycle architecture into a high-throughput execution engine. We explore how pipelining applies assembly-line principles to instruction execution, dramatically improving processor throughput while maintaining individual instruction latency. The lecture examines the three fundamental types of hazards—structural, data, and control—that threaten pipeline efficiency, and discusses practical solutions including forwarding, stalling, and branch prediction that enable real-world pipelined processors to achieve near-ideal performance.</p>


<h2>12.2 Recap: Single-Cycle Performance Limitations</h2>

<h3>12.2.1 Critical Path Problem</h3>

<p><strong>Load Word as Bottleneck:</strong></p>

<ul>
<li>Uses most resources: Instruction Memory → Register File → ALU → Data Memory → Register File</li>
<li>Determines clock period for entire CPU</li>
<li>Forces all other instructions to wait</li>
</ul>

<p><strong>Performance Issue:</strong></p>

<ul>
<li>Most instructions (arithmetic, branch) take less time than load</li>
<li>Jump instruction takes even less time</li>
<li>Clock period set by slowest instruction (load word)</li>
</ul>

<p><strong>Design Principle Violated:</strong></p>

<ul>
<li>"Make the common case fast"</li>
<li>Common case (arithmetic) forced to run slowly</li>
<li>Majority of instructions underutilize available time</li>
</ul>

<h3>12.2.2 Multi-Cycle as First Improvement</h3>

<p><strong>Basic Concept:</strong></p>

<ul>
<li>Divide datapath into stages</li>
<li>Each stage completes in one clock cycle</li>
<li>Shorter clock cycles than single-cycle</li>
</ul>

<p><strong>Five Stages Identified:</strong></p>

<ol>
<li>Instruction Fetch (IF)</li>
<li>Register Reading</li>
<li>ALU Operations</li>
<li>Memory Access</li>
<li>Register Writing</li>
</ol>

<p><strong>Variable Stage Usage:</strong></p>

<ul>
<li>Load: Uses all 5 stages</li>
<li>Most instructions: Skip memory access (4 stages)</li>
<li>Jump: Only 2 stages (manipulating PC)</li>
</ul>

<p><strong>Clock Period Determination:</strong></p>

<ul>
<li>Decided by slowest stage (not slowest instruction)</li>
<li>Adjust work in each stage for balance</li>
<li>Maximize utilization of each clock cycle</li>
</ul>

<p><strong>Limitation:</strong></p>

<ul>
<li>Instruction must finish before next instruction starts</li>
<li>Hardware still idle during many cycles</li>
<li>Room for further improvement</li>
</ul>


<h2>12.3 Pipelining Concept: The Laundry Shop Analogy</h2>

<h3>12.3.1 Non-Pipelined Laundry Shop</h3>

<p><strong>Setup:</strong></p>

<ul>
<li>One employee</li>
<li>Four customers: A, B, C, D</li>
<li>First-come, first-serve basis</li>
<li>Four stages of work per customer:</li>
</ul>
<ol>
<li>Washing: 30 minutes</li>
<li>Drying: 30 minutes</li>
<li>Folding/Ironing: 30 minutes</li>
<li>Packaging: 30 minutes</li>
</ol>

<ul>
<li>Total per customer: 2 hours</li>
</ul>

<p><strong>Sequential Processing:</strong></p>

<img src="../img/Non-Pipelined.jpg" alt="Computer System Abstraction Layers" width="600">


<table>
<thead>
<tr>
<th>Metric</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Total Time</td>
<td>8 hours (6pm to 2am)</td>
</tr>
<tr>
<td>Time per Customer</td>
<td>2 hours</td>
</tr>
<tr>
<td>Shop Closes</td>
<td>2am</td>
</tr>
</tbody>
</table>

<p><strong>Problems:</strong></p>

<ul>
<li>Machines idle while employee works on other stages</li>
<li>Washer idle during drying, folding, packaging</li>
<li>Dryer idle except during drying stage</li>
<li>Tremendous resource underutilization</li>
</ul>

<h3>12.3.2 Pipelined Laundry Shop</h3>

<p><strong>Key Idea:</strong></p>

<ul>
<li>Use idle machines for next customers</li>
<li>Overlap execution of different loads</li>
<li>Parallel processing maximizes hardware utilization</li>
</ul>

<p><strong>Pipelined Schedule:</strong></p>

<img src="../img/Pipelined.jpg" alt="Computer System Abstraction Layers" width="600">

<p><strong>Timeline Analysis:</strong></p>

<ul>
<li>6:00-6:30: A washing (1 station busy)</li>
<li>6:30-7:00: A drying, B washing (2 stations busy)</li>
<li>7:00-7:30: A folding, B drying, C washing (3 stations busy)</li>
<li>7:30-8:00: A packing, B folding, C drying, D washing (4 stations - <strong>ALL BUSY!</strong>)</li>
<li>8:00-8:30: A done, B packing, C folding, D drying, E washing</li>
</ul>

<p><strong>Steady State:</strong></p>

<ul>
<li>Reached at 7:30-8:00 when all 4 stations occupied</li>
<li>Pipeline full</li>
<li>Maximum hardware utilization</li>
<li>One customer finishes every 30 minutes</li>
</ul>

<h3>12.3.3 Performance Analysis</h3>

<p><strong>Time Comparison:</strong></p>

<ul>
<li>Non-pipelined: 8 hours for 4 customers</li>
<li>Pipelined: 3.5 hours for 4 customers</li>
</ul>

<p><strong>Speedup Calculation:</strong></p>

<pre><code>Speedup = Non-pipelined Time / Pipelined Time
        = 8 hours / 3.5 hours
        = 2.3×
</code></pre>


<p><strong>Includes Pipeline Fill Time:</strong></p>

<ul>
<li>First 1.5 hours: Filling pipeline (not all stations busy)</li>
<li>After 1.5 hours: Steady state (all stations busy)</li>
</ul>

<p><strong>Steady State Analysis (ignoring fill time):</strong></p>

<pre><code>Non-pipelined: 2n hours for n loads (2 hours per load)
Pipelined: 0.5n hours for n loads (0.5 hours per load)

Steady State Speedup = 2n / 0.5n = 4×
</code></pre>


<p><strong>Theoretical Maximum Speedup:</strong></p>

<ul>
<li>Equals number of stages</li>
<li>4 stages → 4× speedup maximum</li>
<li>8 stages → 8× speedup maximum (if achievable)</li>
</ul>

<h3>12.3.4 Key Performance Terms</h3>

<p><strong>Latency:</strong></p>

<ul>
<li>Time to complete one individual job</li>
<li>Customer A: Still 2 hours in both cases</li>
<li>Per-instruction time unchanged</li>
</ul>

<p><strong>Throughput:</strong></p>

<ul>
<li>How often one job completes</li>
<li>Non-pipelined: 1 job every 2 hours</li>
<li>Pipelined: 1 job every 30 minutes (steady state)</li>
<li>Throughput is the relevant metric for pipelines</li>
</ul>

<p><strong>Observation:</strong></p>

<ul>
<li>Pipelining doesn't reduce individual job latency</li>
<li>Pipelining dramatically improves throughput</li>
<li>Overall system performance greatly enhanced</li>
</ul>

<p><strong>Analogy Summary:</strong></p>

<ul>
<li>Customers = Instructions</li>
<li>Stages = Pipeline stages</li>
<li>Time saved = Performance improvement</li>
<li>Overlapping execution = Instruction-level parallelism</li>
</ul>


<h2>12.4 MIPS Five-Stage Pipeline</h2>

<h3>12.4.1 Pipeline Stage Definitions</h3>

<h4>Stage 1: Instruction Fetch (IF)</h4>

<ul>
<li>Use current Program Counter (PC)</li>
<li>PC points to next instruction to execute</li>
<li>Access instruction memory</li>
<li>Fetch instruction word</li>
<li>Duration: One clock cycle</li>
</ul>

<h4>Stage 2: Instruction Decode / Register Read (ID)</h4>

<ul>
<li>Decode opcode field</li>
<li>Determine instruction category</li>
<li>Identify remaining bit organization</li>
<li>Extract register addresses</li>
<li>Read register file</li>
<li>Both operations in one stage (workload balancing)</li>
</ul>

<h4>Stage 3: Execution (EX)</h4>

<ul>
<li>Arithmetic/Logic instructions: ALU computes result</li>
<li>Memory instructions: ALU computes address (base + offset)</li>
<li>Branch instructions: ALU performs comparison</li>
<li>One clock cycle</li>
</ul>

<h4>Stage 4: Memory Access (MEM)</h4>

<ul>
<li>Load instructions: Read from data memory</li>
<li>Store instructions: Write to data memory</li>
<li>Other instructions: Skip this stage</li>
<li>One clock cycle</li>
</ul>

<h4>Stage 5: Write Back (WB)</h4>

<ul>
<li>Write result to register file</li>
<li>Source: ALU result (arithmetic) OR memory data (load)</li>
<li>Multiplexer selects appropriate source</li>
<li>One clock cycle</li>
</ul>

<p><strong>Workload Distribution Goal:</strong></p>

<ul>
<li>Evenly distribute work across stages</li>
<li>Minimize clock cycle time</li>
<li>Maximize hardware utilization</li>
</ul>

<h3>12.4.2 Stage Timing Example</h3>

<p><strong>Assumed Component Delays:</strong></p>

<table>
<thead>
<tr>
<th>Component</th>
<th>Delay (picoseconds)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Instruction Fetch</td>
<td>200 ps</td>
</tr>
<tr>
<td>Register Read/Write</td>
<td>100 ps</td>
</tr>
<tr>
<td>ALU Operation</td>
<td>200 ps</td>
</tr>
<tr>
<td>Data Memory Access</td>
<td>200 ps</td>
</tr>
<tr>
<td>Sign Extension</td>
<td>negligible</td>
</tr>
<tr>
<td>Multiplexers</td>
<td>negligible</td>
</tr>
</tbody>
</table>

<p><strong>Single-Cycle Instruction Times:</strong></p>

<table>
<thead>
<tr>
<th>Instruction Type</th>
<th>Stages Used</th>
<th>Total Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>Load Word (LW)</td>
<td>IF+ID+EX+MEM+WB</td>
<td>800 ps</td>
</tr>
<tr>
<td>Store Word (SW)</td>
<td>IF+ID+EX+MEM</td>
<td>700 ps</td>
</tr>
<tr>
<td>R-type (ADD, etc.)</td>
<td>IF+ID+EX+WB</td>
<td>600 ps</td>
</tr>
<tr>
<td>Branch (BEQ)</td>
<td>IF+ID+EX</td>
<td>500 ps</td>
</tr>
</tbody>
</table>

<strong>Load Word Critical Path:</strong> 800 ps determines clock period

<h3>12.4.3 Pipeline Implementation Details</h3>

<p><strong>Clock Cycle Determination:</strong></p>

<ul>
<li>Must accommodate longest stage</li>
<li>Longest stage: 200 ps (IF, ALU, MEM)</li>
<li>Clock cycle: 200 ps</li>
<li>Some stages underutilize cycle (register read/write: 100 ps)</li>
</ul>

<p><strong>Register Read/Write Timing:</strong></p>

<ul>
<li><strong>CRITICAL:</strong> Register write first half, read second half of same clock cycle</li>
<li>Enables same-cycle read-after-write</li>
<li>Prevents data hazards in some cases</li>
</ul>

<p><strong>Stage Alignment to Clock Cycles:</strong></p>

<table>
<thead>
<tr>
<th>Stage</th>
<th>Work</th>
<th>Time</th>
<th>Cycle Time</th>
</tr>
</thead>
<tbody>
<tr>
<td>IF</td>
<td>Instruction Memory read</td>
<td>200 ps</td>
<td>200 ps ✓</td>
</tr>
<tr>
<td>ID</td>
<td>Decode + Register Read</td>
<td>100 ps</td>
<td>200 ps (space left)</td>
</tr>
<tr>
<td>EX</td>
<td>ALU operation</td>
<td>200 ps</td>
<td>200 ps ✓</td>
</tr>
<tr>
<td>MEM</td>
<td>Data Memory access</td>
<td>200 ps</td>
<td>200 ps ✓</td>
</tr>
<tr>
<td>WB</td>
<td>Register write</td>
<td>100 ps</td>
<td>200 ps (first half only)</td>
</tr>
</tbody>
</table>

<p><strong>Space in ID Stage:</strong></p>

<ul>
<li>Register read: 100 ps</li>
<li>Decoding: Fits in remaining 100 ps</li>
<li>Combinational logic for opcode decode</li>
<li>Total: ~200 ps utilized</li>
</ul>

<p><strong>Space in WB Stage:</strong></p>

<ul>
<li>Register write: 100 ps (first half)</li>
<li>Second half: Available for next instruction's register read</li>
</ul>

<h3>12.4.4 Load Word Pipeline Example</h3>

<p><strong>Instruction Stream:</strong> All Load Word instructions</p>

<pre><code class="language-asm6502">LW $1, 0($10)
LW $2, 4($10)
LW $3, 8($10)
LW $4, 12($10)
...
</code></pre>

<p><strong>Pipeline Timing Diagram:</strong></p>

<pre><code>Time (ps):  0-200  200-400  400-600  600-800  800-1000  1000-1200
LW $1:      IF     ID       EX       MEM      WB
LW $2:             IF       ID       EX       MEM       WB
LW $3:                      IF       ID       EX        MEM
LW $4:                               IF       ID        EX
</code></pre>


<p><strong>Single-Cycle Comparison:</strong></p>

<ul>
<li>Non-pipelined: 800 ps per instruction</li>
<li>Pipelined: 200 ps per instruction (after pipeline fills)</li>
</ul>

<p><strong>Throughput Improvement:</strong></p>

<pre><code>Non-pipelined: 1 instruction every 800 ps
Pipelined: 1 instruction every 200 ps

Speedup = 800 / 200 = 4×
</code></pre>


<p><strong>Absolute Time per Instruction:</strong></p>

<ul>
<li>Still ~800 ps (slightly more with alignment overhead)</li>
<li>Latency unchanged or slightly worse</li>
<li>Throughput dramatically improved</li>
</ul>

<h3>12.4.5 Ideal vs Actual Speedup</h3>

<p><strong>Ideal Case (balanced stages):</strong></p>

<pre><code>Time between instructions (pipelined) = Time per instruction (non-pipelined) / Number of stages

Maximum Speedup = Number of Stages
</code></pre>


<p><strong>Actual Implementation:</strong></p>

<ul>
<li>Stages not perfectly balanced</li>
<li>Register operations faster than memory/ALU</li>
<li>Speedup < Number of stages</li>
<li>Example: 5 stages → 4× speedup (not 5×)</li>
</ul>

<p><strong>Reasons for Less Than Ideal:</strong></p>

<ol>
<li>Unbalanced stage delays</li>
<li>Pipeline fill time overhead</li>
<li>Hazards (discussed later)</li>
<li>Added synchronization logic</li>
</ol>


<h2>12.5 MIPS ISA Design for Pipelining</h2>

<h3>12.5.1 Fixed Instruction Length</h3>

<p><strong>MIPS Characteristic:</strong></p>

<ul>
<li>All instructions exactly 32 bits</li>
<li>Same as ARM (also designed for pipelining)</li>
</ul>

<p><strong>Benefits for Pipelining:</strong></p>

<ul>
<li>Simple instruction fetch (always 32 bits)</li>
<li>Simple decode (fixed format)</li>
<li>Bus width fully utilized every time</li>
<li>No variable-width handling logic</li>
</ul>

<p><strong>Alternative (Variable-Length):</strong></p>

<ul>
<li>Complicates fetch stage</li>
<li>Requires width detection logic</li>
<li>May need multiple fetch cycles</li>
<li>Added combinational logic delays</li>
</ul>

<h3>12.5.2 Fewer Regular Instruction Formats</h3>

<p><strong>MIPS Formats:</strong></p>

<ul>
<li>Only 3-4 instruction formats (R, I, J types)</li>
<li>Small opcode field (6 bits)</li>
<li>Regular register field positions</li>
</ul>

<p><strong>Benefits:</strong></p>

<ul>
<li>Fast decoding (small opcode → simple logic)</li>
<li>Fits decode + register read in one stage</li>
<li>Minimal combinational delay</li>
</ul>

<p><strong>Register Field Consistency:</strong></p>

<ul>
<li>RS (bits 21-25): First source register</li>
<li>RT (bits 16-20): Second source / destination</li>
<li>RD (bits 11-15): Destination (R-type)</li>
<li>Same positions across formats</li>
</ul>

<p><strong>Decoding Simplification:</strong></p>

<ul>
<li>Small opcode → simple decode logic</li>
<li>Regular formats → minimal mux complexity</li>
<li>Fast enough for single clock cycle</li>
</ul>

<h3>12.5.3 Separate ALU Operation Field</h3>

<p><strong>Function Field (funct):</strong></p>

<ul>
<li>Bits 0-5: Specifies ALU operation for R-type</li>
<li>Separate from opcode</li>
<li>Only examined for R-type (opcode = 0)</li>
</ul>

<p><strong>Design Rationale:</strong></p>

<ul>
<li>ALU operation determined in EX stage</li>
<li>Opcode used in ID stage</li>
<li>Temporal separation matches pipeline stages</li>
</ul>

<p><strong>Benefit:</strong></p>

<ul>
<li>funct field processed later (EX stage)</li>
<li>Opcode processed early (ID stage)</li>
<li>Separating them simplifies each stage</li>
<li>Avoids large opcode (keeps decode simple)</li>
</ul>

<p><strong>Alternative Design:</strong></p>

<ul>
<li>Include funct in opcode</li>
<li>Larger opcode field needed</li>
<li>More complex decode logic</li>
<li>Slower ID stage</li>
<li>Worse pipeline balance</li>
</ul>

<h3>12.5.4 Load/Store Addressing Mode</h3>

<p><strong>MIPS Addressing:</strong></p>

<ul>
<li>Base register + offset</li>
<li>Address = $rs + immediate</li>
<li>Calculation: Simple addition</li>
</ul>

<p><strong>Pipeline Fit:</strong></p>

<ul>
<li>Address calculation: EX stage (ALU)</li>
<li>Memory access: MEM stage (next cycle)</li>
<li>Clean separation into two stages</li>
</ul>

<p><strong>Design Philosophy:</strong></p>

<ul>
<li>ISA designed with pipeline in mind</li>
<li>Not optimized for single-cycle</li>
<li>Performance through pipelining</li>
</ul>

<p><strong>MIPS vs Other ISAs:</strong></p>

<ul>
<li>MIPS: Designed for pipelining from start</li>
<li>x86: Complex instructions, harder to pipeline</li>
<li>ARM: Similar philosophy to MIPS</li>
<li>RISC principles support pipelining</li>
</ul>


<h2>12.6 Instruction-Level Parallelism (ILP)</h2>

<h3>12.6.1 Parallel Execution Concept</h3>

<p><strong>Definition:</strong></p>

<ul>
<li>Multiple instructions executing simultaneously</li>
<li>Each at different pipeline stage</li>
<li>Overlapping execution</li>
</ul>

<p><strong>Example at Steady State:</strong></p>

<pre><code>Time Window: 800-1000 ps

Instruction A: WB stage (writing result)
Instruction B: MEM stage (memory access)
Instruction C: EX stage (ALU operation)
Instruction D: ID stage (decode, register read)
Instruction E: IF stage (fetch)

Five instructions active simultaneously!
</code></pre>


<p><strong>Instruction-Level Parallelism (ILP):</strong></p>

<ul>
<li>Lowest granularity of parallelism</li>
<li>Inside CPU microarchitecture</li>
<li>Transparent to software</li>
<li>Hardware manages parallelism</li>
</ul>

<h3>12.6.2 Levels of Parallelism</h3>

<p><strong>Instruction-Level Parallelism:</strong></p>

<ul>
<li>Multiple instructions in pipeline</li>
<li>Same program/thread</li>
<li>Within CPU core</li>
<li>Microsecond/nanosecond scale</li>
</ul>

<p><strong>Thread-Level Parallelism:</strong></p>

<ul>
<li>Multiple threads on same core</li>
<li>Context switching</li>
<li>OS-managed</li>
<li>Millisecond scale</li>
</ul>

<p><strong>Program-Level Parallelism:</strong></p>

<ul>
<li>Multiple programs/processes</li>
<li>Multi-core execution</li>
<li>OS-scheduled</li>
<li>Varied time scales</li>
</ul>

<p><strong>Application-Level Parallelism:</strong></p>

<ul>
<li>Distributed computing</li>
<li>Multiple machines</li>
<li>Network communication</li>
<li>Seconds to minutes scale</li>
</ul>

<p><strong>ILP Focus:</strong></p>

<ul>
<li>Fine-grained parallelism</li>
<li>Hardware implementation</li>
<li>Transparent to programmer (mostly)</li>
<li>Foundation for all higher levels</li>
</ul>


<h2>12.7 Pipeline Hazards: Structural Hazards</h2>

<h3>12.7.1 Hazard Definition</h3>

<p><strong>General Concept:</strong></p>

<ul>
<li>Situations preventing next instruction from starting</li>
<li>Violates basic pipelining goal</li>
<li>Reduces throughput</li>
<li>Requires pipeline stalls (bubbles)</li>
</ul>

<p><strong>Three Categories:</strong></p>

<ol>
<li><strong>Structural Hazards:</strong> Hardware resource busy</li>
<li><strong>Data Hazards:</strong> Need data from previous instruction</li>
<li><strong>Control Hazards:</strong> Decision depends on previous result</li>
</ol>

<h3>12.7.2 Structural Hazard: Single Memory</h3>

<p><strong>Scenario:</strong></p>

<ul>
<li>Single memory device for both instructions and data</li>
<li>No separate instruction/data memory</li>
<li>Same device holds program and data</li>
</ul>

<p><strong>Conflict Example:</strong></p>

<pre><code>Time:    0-200   200-400  400-600  600-800
LW $1:   IF      ID       EX       MEM
LW $2:           IF       ID       EX
LW $3:                    IF       ID
LW $4:                             IF  ← CONFLICT!

At 600-800 ps:
• LW $1 needs data memory (MEM stage)
• LW $4 needs instruction memory (IF stage)
• Same physical memory device!
• Cannot access simultaneously
</code></pre>


<p><strong>Problem:</strong></p>

<ul>
<li>Memory can only service one request per cycle</li>
<li>Instruction fetch AND data access conflict</li>
<li>Hardware resource (memory) busy</li>
</ul>

<h3>12.7.3 Pipeline Stall (Bubble)</h3>

<p><strong>Solution: Insert Bubble</strong></p>

<pre><code>Time:    0-200   200-400  400-600  600-800  800-1000  1000-1200
LW $1:   IF      ID       EX       MEM      WB
LW $2:           IF       ID       EX       [BUBBLE]  MEM
LW $3:                    IF       ID       EX        [BUBBLE]
LW $4:                             IF       [BUBBLE]  ID
</code></pre>


<p><strong>Bubble Characteristics:</strong></p>

<ul>
<li>No instruction in that pipeline stage</li>
<li>Like air bubble in water pipeline</li>
<li>Hardware idle for that stage</li>
<li>Wastes one clock cycle</li>
<li>Propagates through pipeline stages</li>
</ul>

<p><strong>Impact:</strong></p>

<ul>
<li>One instruction delayed</li>
<li>Subsequent instructions delayed</li>
<li>Throughput reduced</li>
<li>Performance loss</li>
</ul>

<p><strong>Bubble Analogy:</strong></p>

<ul>
<li>Water pipeline: Continuous flow</li>
<li>Air bubble: Break in flow</li>
<li>Takes time to propagate through</li>
<li>Reduces effective flow rate</li>
</ul>

<h3>12.7.4 Solutions to Structural Hazards</h3>

<p><strong>Solution 1: Separate Memories</strong></p>

<ul>
<li>Instruction memory separate from data memory</li>
<li>Harvard architecture</li>
<li>Simultaneous access possible</li>
<li>No structural hazard</li>
</ul>

<p><strong>Solution 2: Separate Caches</strong></p>

<ul>
<li>Single main memory</li>
<li>Separate instruction cache (I-cache)</li>
<li>Separate data cache (D-cache)</li>
<li>Cache: Fast buffer between CPU and memory</li>
<li>Caches can be accessed simultaneously</li>
<li>Details in future lectures (memory hierarchy)</li>
</ul>

<p><strong>Design Recommendation:</strong></p>

<ul>
<li>Modern processors use separate caches</li>
<li>Necessary for high-performance pipelining</li>
<li>Small area overhead for large performance gain</li>
</ul>


<h2>12.8 Data Hazards</h2>

<h3>12.8.1 Data Hazard Definition</h3>

<p><strong>Concept:</strong></p>

<ul>
<li>Subsequent instruction needs data from previous instruction</li>
<li>Data not yet available (still being computed/written)</li>
<li>Reading too early → wrong value</li>
<li>Writing too early → data corruption</li>
</ul>

<p><strong>Example:</strong></p>

<pre><code class="language-asm6502">ADD $s0, $t0, $t1      # $s0 = $t0 + $t1
SUB $t2, $s0, $t3      # $t2 = $s0 - $t3 (uses $s0 from ADD)
</code></pre>


<p><strong>Problem:</strong></p>

<ul>
<li>ADD computes $s0 value in EX stage</li>
<li>SUB needs $s0 value in ID stage (register read)</li>
<li>Timing mismatch</li>
</ul>

<h3>12.8.2 Data Hazard Example Analysis</h3>

<p><strong>Instruction Sequence:</strong></p>

<pre><code class="language-asm6502">ADD $s0, $t0, $t1
SUB $t2, $s0, $t3
</code></pre>

<p><strong>Pipeline Without Stalls:</strong></p>

<pre><code>Time:    0-200   200-400  400-600  600-800  800-1000
ADD:     IF      ID       EX       MEM      WB
SUB:             IF       ID       EX       MEM
                          ↑
                 Reads $s0 here (old value!)

                          ADD writes $s0 here ↓
</code></pre>


<p><strong>Problem Timeline:</strong></p>

<ul>
<li>200-400: ADD reads $t0, $t1; SUB fetched</li>
<li>400-600: ADD computes in ALU; SUB reads registers (gets OLD $s0!)</li>
<li>600-800: ADD result available but not in register yet</li>
<li>800-1000: ADD writes $s0 to register (first half of cycle)</li>
</ul>

<p>SUB reads $s0 at 400-600, but correct value not available until 800-1000!</p>

<h3>12.8.3 Solution 1: Pipeline Stalls</h3>

<p><strong>Insert Two Bubbles:</strong></p>

<pre><code>Time:    0-200   200-400  400-600  600-800  800-1000  1000-1200  1200-1400
ADD:     IF      ID       EX       MEM      WB
[BUBBLE]                  IF       [BUBBLE] [BUBBLE]
[BUBBLE]                           IF       [BUBBLE]
SUB:                                         IF        ID
</code></pre>


<p><strong>Result:</strong></p>

<ul>
<li>SUB fetched at 1000-1200</li>
<li>SUB reads registers at 1200-1400 (second half at 1200)</li>
<li>ADD writes $s0 at 800-1000 (first half at 800)</li>
<li>Sufficient time gap: Correct value available</li>
</ul>

<p><strong>Cost:</strong></p>

<ul>
<li>Two clock cycles wasted</li>
<li>Throughput reduced</li>
<li>Performance penalty</li>
</ul>

<p><strong>Critical Timing:</strong></p>

<ul>
<li>Register write: First half of WB cycle</li>
<li>Register read: Second half of ID cycle</li>
<li>Enables back-to-back reading of just-written value</li>
</ul>

<h3>12.8.4 Solution 2: Forwarding (Bypassing)</h3>

<p><strong>Key Observation:</strong></p>

<ul>
<li>ADD result available after EX stage (400-600)</li>
<li>Result at ALU output</li>
<li>Not yet written to register file</li>
<li>But SUB's ALU operation at 600-800</li>
<li>Can forward ALU output directly to ALU input!</li>
</ul>

<p><strong>Forwarding Logic:</strong></p>

<pre><code>Time:    0-200   200-400  400-600  600-800  800-1000
ADD:     IF      ID       EX       MEM      WB
SUB:             IF       ID       EX       MEM
                          ↑        ↑
                 Read regs   Use forwarded value!
</code></pre>


<p><strong>Implementation:</strong></p>

<ul>
<li>Multiplexer at ALU input</li>
<li>Selects between:</li>
</ul>

<pre><code>- Register file output (normal path)
- Forwarded value from previous ALU output
</code></pre>

<ul>
<li>Control logic detects dependency</li>
<li>Routes correct value</li>
</ul>

<p><strong>Benefit:</strong></p>

<ul>
<li>Eliminates two stalls</li>
<li>No performance penalty</li>
<li>Requires additional hardware:</li>
</ul>

<pre><code>- Forwarding multiplexers
- Forwarding detection logic
- Forwarding paths (wires)
- Pipeline registers to hold values
</code></pre>

<p><strong>Complexity:</strong></p>

<ul>
<li>Careful synchronization required</li>
<li>Detect true dependencies</li>
<li>Avoid false positives</li>
<li>Additional control signals</li>
</ul>

<p><strong>Result:</strong></p>

<ul>
<li>SUB can execute immediately after ADD</li>
<li>No stalls needed</li>
<li>Correct value forwarded</li>
</ul>

<h3>12.8.5 Load-Use Data Hazard</h3>

<p><strong>Special Case:</strong></p>

<pre><code class="language-asm6502">LW  $s0, 0($t0)        # Load from memory into $s0
SUB $t2, $s0, $t3      # Use $s0 immediately
</code></pre>

<p><strong>Problem:</strong></p>

<ul>
<li>Load result available after MEM stage (data from memory)</li>
<li>SUB needs value in EX stage</li>
<li>Even forwarding can't help!</li>
</ul>

<p><strong>Timeline:</strong></p>

<pre><code>Time:    0-200   200-400  400-600  600-800  800-1000
LW:      IF      ID       EX       MEM      WB
SUB:             IF       ID       EX       MEM
                          ↑        ↑
                 Need value   Value first available here!
</code></pre>

<p>LW result available at 600-800, but SUB's EX at 600-800 (simultaneous!)</p>

<p><strong>Unavoidable Stall:</strong></p>

<pre><code>Time:    0-200   200-400  400-600  600-800  800-1000  1000-1200
LW:      IF      ID       EX       MEM      WB
[BUBBLE]                  IF       [BUBBLE] ID
SUB:                                         IF        ID
</code></pre>


<p><strong>One stall bubble required:</strong></p>

<ul>
<li>Cannot be eliminated by forwarding</li>
<li>Can forward from MEM to EX (saves one stall vs two)</li>
<li>But at least one stall unavoidable</li>
</ul>

<h3>12.8.6 Compiler Solution: Code Reordering</h3>

<p><strong>C Code Example:</strong></p>

<pre><code class="language-c">a = b + e;
c = b + f;
</code></pre>

<p><strong>Naive Assembly (Load-Use Hazards):</strong></p>

<pre><code class="language-asm6502">LW   $t1, 0($t0)    # Load b into $t1
LW   $t2, 4($t0)    # Load e into $t2
ADD  $t3, $t1, $t2  # a = b + e ← HAZARD: uses $t2 immediately after LW
SW   $t3, 8($t0)    # Store a

LW   $t4, 12($t0)   # Load f into $t4
ADD  $t5, $t1, $t4  # c = b + f ← HAZARD: uses $t4 immediately after LW
SW   $t5, 16($t0)   # Store c
</code></pre>

<p><strong>Total:</strong> 7 instructions + 2 stalls = 9 clock cycles</p>

<p><strong>Optimized Assembly (Reordered):</strong></p>

<pre><code class="language-asm6502">LW   $t1, 0($t0)    # Load b into $t1
LW   $t2, 4($t0)    # Load e into $t2
LW   $t4, 12($t0)   # Load f into $t4 ← Moved here!
ADD  $t3, $t1, $t2  # a = b + e ← No hazard! $t2 available
SW   $t3, 8($t0)    # Store a ← Moved here!
ADD  $t5, $t1, $t4  # c = b + f ← No hazard! $t4 available
SW   $t5, 16($t0)   # Store c
</code></pre>

<p><strong>Total:</strong> 7 instructions + 0 stalls = 7 clock cycles</p>

<p><strong>Technique:</strong></p>

<ul>
<li>Load f earlier (between loading b and e)</li>
<li>Fills stall slot with useful work</li>
<li>Store a before second ADD (fills another gap)</li>
<li>No bubbles needed</li>
</ul>

<strong>Savings:</strong> 2 clock cycles (22% improvement)

<p><strong>Compiler Responsibility:</strong></p>

<ul>
<li>Analyze dependencies</li>
<li>Reorder instructions safely</li>
<li>Fill stall slots with independent instructions</li>
<li>Maintain program semantics</li>
</ul>

<p><strong>Programmer Awareness:</strong></p>

<ul>
<li>Understand pipeline behavior</li>
<li>Write code amenable to reordering</li>
<li>Separate dependent instructions when possible</li>
<li>Help compiler optimize</li>
</ul>


<h2>12.9 Control Hazards</h2>

<h3>12.9.1 Control Hazard Definition</h3>

<p><strong>Concept:</strong></p>

<ul>
<li>Branch/Jump outcome determines next instruction</li>
<li>Decision depends on previous computation</li>
<li>Can't fetch next instruction until decision made</li>
<li>Pipeline must wait</li>
</ul>

<p><strong>Example:</strong></p>

<pre><code class="language-asm6502">BEQ $1, $2, target     # Branch if $1 == $2
ADD $3, $4, $5         # Next sequential instruction
...
target: SUB $6, $7, $8 # Branch target
</code></pre>


<p><strong>Which instruction to fetch after BEQ?</strong></p>

<ul>
<li>ADD if branch NOT taken</li>
<li>SUB if branch IS taken</li>
<li>Decision requires comparison: $1 vs $2</li>
</ul>

<h3>12.9.2 Branch Execution in Pipeline</h3>

<p><strong>Branch Instruction:</strong></p>

<pre><code class="language-asm6502">BEQ $1, $2, 40         # Branch 40 instructions ahead if equal
</code></pre>


<p><strong>Pipeline Stages:</strong></p>

<ol>
<li>IF: Fetch BEQ instruction</li>
<li>ID: Read $1, $2 from register file</li>
<li>EX: ALU compares (subtract $2 from $1, check zero flag)</li>
<li>Result available after EX stage</li>
</ol>

<p><strong>Problem:</strong></p>

<ul>
<li>Next instruction fetch at cycle 2 (IF for next instruction)</li>
<li>Branch outcome known at cycle 3 (after EX)</li>
<li>Must guess which instruction to fetch!</li>
</ul>

<p><strong>Without Optimization:</strong></p>

<pre><code>Time:    0-200   200-400  400-600  600-800
BEQ:     IF      ID       EX       MEM
???:             IF       ???
</code></pre>

<p>Two bubbles required if wait for outcome</p>

<h3>12.9.3 Solution 1: Early Branch Resolution</h3>

<p><strong>Add Hardware in ID Stage:</strong></p>

<ul>
<li>Small adder for comparison</li>
<li>Compute branch condition early (ID instead of EX)</li>
<li>Subtract $1 - $2 in ID stage</li>
<li>Parallel to register read</li>
</ul>

<p><strong>Modified Pipeline:</strong></p>

<pre><code>Time:    0-200   200-400  400-600
BEQ:     IF      ID       EX
                 ↑
         Decision here!
Next:            IF
</code></pre>


<p><strong>Benefit:</strong></p>

<ul>
<li>Decision after ID (one cycle earlier)</li>
<li>Only one bubble needed (vs two)</li>
<li>Better performance</li>
</ul>

<p><strong>Cost:</strong></p>

<ul>
<li>Additional adder hardware</li>
<li>Extra combinational logic in ID stage</li>
<li>More complex ID stage</li>
</ul>

<p><strong>Limitation:</strong></p>

<ul>
<li>Still one unavoidable stall</li>
<li>Can't know outcome in same cycle as fetch</li>
</ul>

<h3>12.9.4 Solution 2: Branch Prediction</h3>

<p><strong>Static Branch Prediction:</strong></p>

<ul>
<li>Guess branch outcome</li>
<li>Fetch based on guess</li>
<li>If correct: No penalty</li>
<li>If wrong: Discard fetched instruction, fetch correct one</li>
</ul>

<p><strong>Strategy: Predict Not Taken</strong></p>

<ul>
<li>Assume branch will NOT be taken</li>
<li>Always fetch PC + 4 (sequential instruction)</li>
<li>Proceed normally if correct</li>
<li>Stall and correct if wrong</li>
</ul>

<p><strong>Example (Prediction Correct):</strong></p>

<pre><code class="language-asm6502">ADD  $3, $4, $5
BEQ  $1, $2, 14        # Actually NOT taken
LW   $8, 0($9)         # Fetch this (prediction: not taken)
</code></pre>

<p><strong>Timeline:</strong></p>

<pre><code>Time:    0-200   200-400  400-600  600-800
ADD:     IF      ID       EX       MEM
BEQ:             IF      ID       EX
LW:                      IF       ID
                         ↑ Fetched based on prediction
</code></pre>


<p>At 400-600 (after BEQ's ID):</p>

<ul>
<li>Determine branch NOT taken</li>
<li>Prediction correct!</li>
<li>LW continues normally</li>
<li>No stall!</li>
</ul>

<p><strong>Example (Prediction Incorrect):</strong></p>

<pre><code class="language-asm6502">ADD  $3, $4, $5
BEQ  $1, $2, 14        # Actually IS taken
LW   $8, 0($9)         # Fetched (but shouldn't execute)
...
target: SUB $6, $7, $8 # Should execute this instead
</code></pre>

<p><strong>Timeline:</strong></p>

<pre><code>Time:    0-200   200-400  400-600  600-800
ADD:     IF      ID       EX       MEM
BEQ:             IF      ID       EX
LW:                      IF       [DISCARD]
SUB:                              IF
</code></pre>
<p>BEQ:             IF      ID       EX</p>
<p>LW:                      IF       [DISCARD]</p>
<p>SUB:                              IF</p>
</code>`<code>


<p>At 400-600 (after BEQ's ID):</p>

<ul>
<li>Determine branch IS taken</li>
<li>Prediction wrong!</li>
<li>Discard LW (clear pipeline stage)</li>
<li>Fetch SUB from branch target</li>
<li>One bubble inserted</li>
</ul>

<p><strong>Result Analysis:</strong></p>

<ul>
<li>Correct prediction: Save one cycle</li>
<li>Incorrect prediction: Same as no prediction (one stall)</li>
<li>Net benefit if prediction often correct</li>
<li>No additional penalty for wrong guess</li>
</ul>

<h3>12.9.5 Static Branch Prediction Strategies</h3>

<p><strong>Simple Static: Always Predict Not Taken</strong></p>

<ul>
<li>Fixed prediction</li>
<li>Ignore branch type</li>
<li>Ignore branch history</li>
<li>Simple hardware</li>
</ul>

<p><strong>Program Behavior-Based Static:</strong></p>

<ul>
<li>Analyze typical branch patterns</li>
<li>Make predictions based on code structure</li>
</ul>

<p><strong>Backward Branches:</strong></p>

<ul>
<li>Usually taken</li>
<li>Example: Loops</li>
</ul>

<pre><code class="language-asm6502">loop:
    ...
    BEQ $t0, $zero, loop   # Backward branch
</code></pre>

<ul>
<li>Loop iterations: Branch taken many times</li>
<li>Loop exit: Branch not taken once</li>
<li>Prediction: Taken → Correct most of time</li>
</ul>

<p><strong>Forward Branches:</strong></p>

<ul>
<li>Usually not taken</li>
<li>Example: If statements</li>
</ul>

<pre><code class="language-asm6502">BEQ $t0, $zero, skip
...                      # True case
skip:
...                      # After if
</code></pre>

<ul>
<li>True case: Branch not taken</li>
<li>False case: Branch taken</li>
<li>Prediction depends on code style</li>
</ul>

<p><strong>Strategy: Backward Taken, Forward Not Taken</strong></p>

<ul>
<li>90%+ accuracy possible</li>
<li>Based on empirical program analysis</li>
<li>Requires code analysis</li>
</ul>

<h3>12.9.6 Dynamic Branch Prediction</h3>

<p><strong>Concept:</strong></p>

<ul>
<li>Hardware learns branch behavior</li>
<li>Predicts based on history</li>
<li>Adapts to current code execution</li>
<li>Not fixed prediction</li>
</ul>

<p><strong>Branch History Table:</strong></p>

<ul>
<li>Hardware table storing recent branch outcomes</li>
<li>Indexed by branch instruction address</li>
<li>Each entry: Branch taken or not taken recently</li>
<li>Predicts based on recent behavior</li>
</ul>

<p><strong>Simple 1-Bit Predictor:</strong></p>

<ul>
<li>One bit per branch: Last outcome</li>
<li>Predict same as last time</li>
<li>Updates after each execution</li>
</ul>

<p><strong>Example:</strong></p>

<pre><code>Loop iteration 1: Taken → Predict taken next
Loop iteration 2: Taken → Predict taken next
...
Loop iteration 100: Taken → Predict taken next
Loop exit: Not taken → Predict not taken next (wrong for next loop!)
</code></pre>

<p>Problem: Wrong twice per loop (entry and exit)</p>

<p><strong>2-Bit Saturating Counter:</strong></p>

<ul>
<li>Two bits per branch: State machine</li>
<li>Four states:</li>
</ul>

<pre><code>- 00: Strongly not taken
- 01: Weakly not taken
- 10: Weakly taken
- 11: Strongly taken
</code></pre>

<ul>
<li>Change prediction after two consecutive wrong predictions</li>
<li>More stable</li>
</ul>

<p><strong>Advanced Predictors:</strong></p>

<ul>
<li>Correlating predictors (look at multiple branches)</li>
<li>Two-level adaptive predictors</li>
<li>Tournament predictors (combine multiple algorithms)</li>
<li>Very high accuracy (>95%)</li>
</ul>

<p><strong>Hardware Cost:</strong></p>

<ul>
<li>Branch history table (memory)</li>
<li>Prediction logic (comparators, counters)</li>
<li>Update logic</li>
<li>Worthwhile for performance gain</li>
</ul>


<h2>12.10 Summary and Key Concepts</h2>

<h3>12.10.1 Pipelining Benefits</h3>

<p><strong>Performance Improvement:</strong></p>

<ul>
<li>Throughput increased by number of stages</li>
<li>5-stage pipeline → 4-5× speedup</li>
<li>Latency unchanged or slightly worse</li>
<li>Overlapping execution key</li>
</ul>

<p><strong>Hardware Utilization:</strong></p>

<ul>
<li>All stages active in steady state</li>
<li>Parallel processing</li>
<li>Maximum efficiency</li>
</ul>

<h3>12.10.2 Pipeline Challenges</h3>

<p><strong>Hazards:</strong></p>

<ol>
<li><strong>Structural:</strong> Hardware resource conflicts</li>
<li><strong>Data:</strong> Instruction dependencies</li>
<li><strong>Control:</strong> Branch/jump decisions</li>
</ol>

<p><strong>Solutions:</strong></p>

<ul>
<li>Structural: Separate memories/caches</li>
<li>Data: Forwarding, stalls, code reordering</li>
<li>Control: Early resolution, branch prediction</li>
</ul>

<h3>12.10.3 MIPS Design Philosophy</h3>

<p><strong>ISA Designed for Pipelining:</strong></p>

<ul>
<li>Fixed 32-bit instruction length</li>
<li>Regular instruction formats</li>
<li>Separate funct field</li>
<li>Simple addressing modes</li>
<li>Balanced pipeline stages</li>
</ul>

<p><strong>Performance Through Hardware:</strong></p>

<ul>
<li>Pipelining fundamental to MIPS</li>
<li>Not optimized for single-cycle</li>
<li>Hardware complexity for software simplicity</li>
</ul>

<h3>12.10.4 Key Takeaways</h3>

<p>1. Pipelining improves throughput, not latency</p>
<p>2. Steady state determines peak performance</p>
<p>3. Pipeline fill time overhead for small programs</p>
<p>4. Hazards reduce pipelining efficiency</p>
<p>5. Forwarding eliminates many data hazards</p>
<p>6. Load-use hazard always requires one stall</p>
<p>7. Branch prediction crucial for control flow</p>
<p>8. Compiler optimization reduces stalls</p>
<p>9. ISA design significantly impacts pipeline efficiency</p>
<p>10. ILP fundamental to modern processor performance</p>


<h2>12.11 Important Formulas and Metrics</h2>

<h3>Speedup Calculation</h3>

<pre><code>Speedup = Non-pipelined Time / Pipelined Time

Ideal Speedup = Number of Pipeline Stages

Actual Speedup = Number of Stages / (1 + Hazard Impact)
</code></pre>

<h3>Throughput</h3>

<pre><code>Throughput = 1 instruction / Clock Period

Throughput Improvement = Clock Period (non-pipelined) / Clock Period (pipelined)
</code></pre>

<h3>Pipeline Performance</h3>

<pre><code>Time = (Number of Instructions + Stages - 1) × Clock Period

CPI (Cycles Per Instruction) = 1 + Stall Cycles per Instruction

Effective CPI = 1 + (Structural Stalls + Data Stalls + Control Stalls)
</code></pre>

<h3>Branch Prediction Accuracy</h3>

<pre><code>Accuracy = Correct Predictions / Total Branches

Stall Reduction = Accuracy × Cycles Saved per Correct Prediction
</code></pre>



<h2>Key Takeaways</h2>

<ol>
<li><strong>Pipelining improves throughput, not latency</strong>—individual instructions take same or longer time, but more instructions complete per unit time.</li>

<li><strong>Five-stage MIPS pipeline</strong>: Instruction Fetch (IF), Instruction Decode (ID), Execute (EX), Memory Access (MEM), Write-Back (WB).</li>

<li><strong>Ideal speedup equals number of stages</strong>—five-stage pipeline theoretically achieves 5× speedup over single-cycle design.</li>

<li><strong>Assembly line analogy clarifies concept</strong>—like manufacturing, each stage works on different item simultaneously for maximum efficiency.</li>

<li><strong>Pipeline registers store intermediate results</strong> between stages, enabling independent operation and preventing data corruption.</li>

<li><strong>Three hazard types threaten pipeline efficiency</strong>: Structural (resource conflicts), Data (register dependencies), Control (branch/jump delays).</li>

<li><strong>Structural hazards resolved by hardware duplication</strong>—separate instruction and data caches eliminate memory access conflicts.</li>

<li><strong>Data hazards occur when instructions depend on previous results</strong>—forwarding (bypassing) allows ALU results to skip write-back stage.</li>

<li><strong>Forwarding paths connect pipeline stages directly</strong>, enabling result use before register file write completes.</li>

<li><strong>Load-use hazard requires one-cycle stall</strong>—memory data unavailable in time for immediate ALU use even with forwarding.</li>

<li><strong>Compiler code reordering can eliminate some stalls</strong>—moving independent instructions into load delay slots maintains pipeline flow.</li>

<li><strong>Control hazards arise from branch/jump instructions</strong>—don't know next PC until branch resolves in third cycle.</li>

<li><strong>Branch delay of 3 cycles</strong> in basic pipeline—fetch/decode/execute complete before decision known, wasting 3 instruction slots.</li>

<li><strong>Early branch resolution reduces penalty</strong>—dedicated comparison hardware in ID stage cuts delay to 1 cycle.</li>

<li><strong>Static branch prediction</strong> assumes direction (e.g., always not-taken)—simple but limited effectiveness.</li>

<li><strong>Dynamic branch prediction</strong> learns patterns from history—branch target buffer with 2-bit saturating counters achieves >90% accuracy.</li>

<li><strong>Two-bit counters prevent single misprediction disruption</strong>—requires two wrong predictions to change direction, handling loop patterns well.</li>

<li><strong>Pipeline performance</strong> = 1 CPI + Structural Stalls + Data Stalls + Control Stalls—minimizing hazards approaches ideal throughput.</li>

<li><strong>Modern processors use sophisticated prediction</strong>—multi-level predictors, pattern history tables, and return address stacks minimize control hazards.</li>

<li><strong>Pipeline complexity trades off with performance</strong>—deeper pipelines increase throughput but amplify hazard penalties and design difficulty.</li>
</ol>

<h2>Summary</h2>

<p>Pipelining revolutionizes processor performance by applying manufacturing assembly-line principles to instruction execution, allowing multiple instructions to occupy different pipeline stages simultaneously. The five-stage MIPS pipeline (IF, ID, EX, MEM, WB) theoretically achieves 5× speedup by keeping all hardware components busy every cycle, transforming the inefficient single-cycle design where most hardware sat idle most of the time. However, three hazard types threaten this ideal performance: structural hazards from resource conflicts (solved by hardware duplication like separate instruction and data caches), data hazards from register dependencies (addressed by forwarding paths that bypass results directly between stages, though load-use cases still require one-cycle stalls), and control hazards from branches that don't resolve until the third cycle (mitigated by early branch resolution hardware, static prediction strategies, and sophisticated dynamic branch predictors using two-bit saturating counters that achieve over 90% accuracy). The effectiveness of forwarding demonstrates how careful hardware design can eliminate most data hazard stalls, while compiler optimizations like instruction reordering can fill remaining delay slots with useful work. Branch prediction evolution from simple static schemes to complex dynamic predictors with branch target buffers reflects the critical importance of minimizing control hazards in modern high-performance processors. Pipeline registers between stages serve as the crucial mechanism enabling independent stage operation, storing intermediate results and control signals while preventing data corruption across instruction overlaps. While pipelining introduces significant design complexity compared to single-cycle implementations, the dramatic performance improvements—approaching 5× speedup in practice—justify this added sophistication, making pipelining universal in modern processor architectures from embedded systems to supercomputers. Understanding these hazards and their solutions provides essential foundation for comprehending real-world processor implementations and the tradeoffs between pipeline depth, clock frequency, and hazard penalties that define contemporary computer architecture.</p>

            
            <div class="lecture-nav">
                <a href="lecture-11.html" class="nav-btn">← Previous Lecture</a>
                <a href="lecture-13.html" class="nav-btn">Next Lecture →</a>
            </div>
        </div>
    </main>

    <footer>
        <div class="container">
            <p>&copy; 2025 CO224 Computer Architecture Lecture Series. All rights reserved.</p>
            <p>Department of Computer Engineering, University of Peradeniya</p>
        </div>
    </footer>
</body>
</html>
