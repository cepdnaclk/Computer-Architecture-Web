\section{Lecture 18: Virtual Memory}

\emph{By Dr. Isuru Nawinne}

\subsection{Introduction}

Virtual memory represents one of the most elegant abstractions in computer architecture, creating a layer between physical memory hardware and the memory view presented to programs. This lecture explores how virtual memory enables programs to use more memory than physically available by treating main memory as a cache for disk storage, supports safe execution of multiple concurrent programs through address space isolation, and provides memory protection mechanisms preventing programs from corrupting each other's data. We examine page tables, translation lookaside buffers (TLBs), page faults, and the critical design decisions that make virtual memory both practical and performant despite the enormous speed gap between RAM and disk storage.

\subsection{Introduction to Virtual Memory}

Virtual memory allows programs to use more memory than physically available by using main memory as a cache for secondary storage.

\subsubsection{Key Purposes of Virtual Memory}

\begin{enumerate}
\item \textbf{Allow programs to use more memory than actually available}
\item \textbf{Support multiple programs running simultaneously on a CPU}
\item \textbf{Enable safe and efficient memory sharing between programs}
\item \textbf{Ensure programs only access their allocated memory}

\subsection{CPU Word Size and Address Space}

The relationship between CPU word size and addressable memory determines the maximum amount of memory that can be addressed.

\subsubsection{Address Space by CPU Word Sizerd Size}

\paragraph{8-bit CPU}

\begin{itemize}
\item \textbf{Maximum addressable memory}: 256 bytes (2^8)

\paragraph{16-bit CPU}

\begin{itemize}
\item \textbf{Maximum addressable memory}: 64 kilobytes (2^16)

\paragraph{32-bit CPU}

\begin{itemize}
\item \textbf{Maximum addressable memory}: 4 gigabytes (2^32)
\item Became mainstream in early 1980s
\item Was replaced when systems started reaching 4 GB memory limit

\paragraph{64-bit CPU}

\begin{itemize}
\item \textbf{Maximum addressable memory}: 16 exabytes (2^64)
\item About 16 million gigabytes
\item Current mainstream word size
\item Became mainstream around 2002-2003

\subsubsection{Historical Pattern}

\begin{itemize}
\item Maximum address space sizes were always much larger than commonly used RAM sizes
\item Architectures were replaced when high-end systems started reaching the address space limits
\item Personal computers typically had much less memory than the theoretical maximum

\subsection{Virtual vs Physical Addresses}

\subsubsection{Virtual Address}

\begin{itemize}
\item \textbf{Address generated by CPU}
\item Refers to entire theoretical address space
\item CPU thinks it has access to full address space
\item In 64-bit CPU: can address up to 16 exabytes

\subsubsection{Physical Address}

\begin{itemize}
\item \textbf{Actual address in real memory (RAM)}
\item Much smaller range than virtual addresses
\item Typical modern RAM: 8-16 GB (much less than 16 exabytes)

\subsubsection{Address Translation}

\begin{itemize}
\item Virtual addresses must be translated to physical addresses
\item Translation required every time memory is accessed
\item Main mechanism for making virtual memory work

\subsection{Memory Hierarchy with Virtual Memory}

Complete hierarchy from top to bottom:

\begin{enumerate}
\item \textbf{CPU} (generates virtual addresses, thinks memory is large and fast)
\item \textbf{Cache} (virtually or physically addressed)
\item \textbf{Main Memory} (acts as cache for secondary storage)
\item \textbf{Secondary Storage/Disk} (contains all pages)

CPU accesses cache directly. Main memory acts as cache for disk, not just a second level cache - requires additional mechanisms.

\subsection{Terminology}

\subsubsection{CPU Level}

\begin{itemize}
\item \textbf{Accesses}: Words (1, 4, or 8 bytes)
\item Hit/Miss terminology used

\subsubsection{Cache Level}

\begin{itemize}
\item \textbf{Transfers}: Blocks (16-256 bytes typically)
\item Hit/Miss terminology used

\subsubsection{Memory Level}

\begin{itemize}
\item \textbf{Transfers}: Pages (1 KB to 64 KB typically)
\item \textbf{Page Hit}: Page is present in memory
\item \textbf{Page Fault}: Page is not present in memory (not "miss")

\subsection{Access Latencies}

Understanding the latency differences is crucial for virtual memory design:

\begin{itemize}
\item \textbf{Cache Hit}: Under 1 cycle
\item \textbf{Cache Miss} (accessing main memory): 10-100 cycles
\item \textbf{Page Fault} (accessing disk): ~1 million cycles
\item Extremely large penalty
\item Influences design decisions significantly
\item Page faults handled in software by OS due to large penalty

![Access Latencies](img/Virtual_mem_cycles.jpg)

\subsection{Virtual and Physical Address Structure}

\subsubsection{Example with 32-bit Addressesdresses}

\paragraph{Virtual Address (32 bits)}

\begin{itemize}
\item \textbf{Virtual Page Number}: 22 bits (most significant)
\item \textbf{Page Offset}: 10 bits (least significant)
\item \textbf{Virtual address space}: 4 GB
\item \textbf{Number of virtual pages}: 2^22 pages
\item \textbf{Page size}: 2^10 = 1 KB

\paragraph{Physical Address (28 bits)}

\begin{itemize}
\item \textbf{Physical Page Number (Frame Number)}: 18 bits (most significant)
\item \textbf{Page Offset}: 10 bits (least significant)
\item \textbf{Physical address space}: 256 MB
\item \textbf{Number of frames}: 2^18 frames
\item \textbf{Page size}: 1 KB (same as virtual)

\subsubsection{Key Points}

\begin{itemize}
\item Page offset has same number of bits in virtual and physical addresses
\item Physical address space is smaller than virtual address space
\item Memory contains "frames" where pages can be placed
\item Frame = slot in memory that can hold a page

\subsection{Supporting Multiple Programs}

Multiple programs can run simultaneously by sharing physical memory:

\subsubsection{Each Program}

\begin{itemize}
\item Has its own virtual address space
\item Thinks it has entire memory to itself
\item CPU switches between programs quickly
\item Creates impression of simultaneous execution

\subsubsection{Memory Sharing}

\begin{itemize}
\item Physical memory contains active pages from all running programs
\item Each program's virtual pages map to different physical frames
\item Operating system ensures programs only access their own memory

\subsubsection{Example}

\begin{itemize}
\item Program 1 virtual address space: 8 virtual pages
\item Program 2 virtual address space: 8 virtual pages
\item Physical memory: Only 4 frames available
\item Active pages from both programs share the 4 frames
\item Same virtual page number from different programs can map to different physical frames

\subsection{Page Table}

The page table is a data structure stored in memory that contains address translations.

\subsubsection{Purpose}

\begin{itemize}
\item Stores virtual-to-physical address translations
\item One page table per program
\item Contains entries for ALL virtual pages (not just active ones)

\subsubsection{Page Table Entry Contents}

\begin{enumerate}
\item \textbf{Physical Page Number} (main component)
\item \textbf{Valid Bit}: Is the page currently in memory?
\item 1 = Page is in memory (translation valid)
\item 0 = Page not in memory (page fault)
\item \textbf{Dirty Bit}: Has page been modified?
\item 1 = Page modified, inconsistent with disk
\item 0 = Page not modified, consistent with disk
\item \textbf{Additional bits}: Access permissions, memory protection status

\subsubsection{Finding Page Table}

\begin{itemize}
\item Page tables stored at fixed locations in memory
\item \textbf{Page Table Base Register (PTBR)}: Special CPU register storing starting address of active page table
\item When CPU switches programs, OS updates PTBR to point to correct page table

\subsection{Address Translation Process}

![Address Translation Process](img/Virtual_Mem_Translation.jpg)

Steps to access memory:

\begin{enumerate}
\item \textbf{CPU generates virtual address} (virtual page number + page offset)
\item \textbf{Access page table} using PTBR + virtual page number as index
\item \textbf{Read page table entry}:
\item If valid bit = 0: Page fault (handled by OS)
\item If valid bit = 1: Read physical page number
\item \textbf{Construct physical address}: Physical page number + page offset
\item \textbf{Access physical memory} with physical address
\item \textbf{Return data to CPU}

\subsubsection{Memory Accesses Required}

\begin{itemize}
\item One access for page table
\item One access for actual data
\item \textbf{Total}: Two memory accesses per data access

\subsection{Page Table Size Calculation}

\subsubsection{Example: 4 GB Virtual, 1 GB Physical, 1 KB PagesKB Pages}

\paragraph{Number of Entries}

\begin{itemize}
\item Virtual address: 32 bits
\item Page offset: 10 bits (for 1 KB pages)
\item Virtual page number: 22 bits
\item \textbf{Number of entries}: 2^22 = ~4 million entries

\paragraph{Entry Size}

\begin{itemize}
\item Physical address: 30 bits (for 1 GB)
\item Page offset: 10 bits
\item Physical page number: 20 bits
\item Valid bit: 1 bit
\item Dirty bit: 1 bit
\item Total needed: 22 bits
\item Actual storage: 32 bits (word-aligned)
\item \textbf{Size per entry}: 4 bytes

\paragraph{Total Page Table Size}

\begin{itemize}
\item 4 bytes $\times$ 2^22 entries = \textbf{16 MB}
\item Significant memory overhead for each program

\subsection{Write Policy for Virtual Memory}

\subsubsection{Write-Through: NOT USED}

\begin{itemize}
\item Would require writing to disk on every write
\item 1 million cycle penalty unacceptable
\item Not a good design decision

\subsubsection{Write-Back: USED (Standard Policy)}

\begin{itemize}
\item Writes only update memory
\item Dirty bit tracks modified pages
\item Only write to disk when:
\item Page is evicted from memory
\item Page's dirty bit is 1
\item Minimizes disk accesses

\subsection{Placement Policy}

\subsubsection{Fully Associative Placement}

\begin{itemize}
\item Any page from disk can go to any frame in memory
\item Memory treated as one large set containing all frames
\item No direct mapping or set restrictions
\item Maximizes flexibility in page placement
\item Reduces page faults

\subsubsection{Why Fully Associative?}

\begin{itemize}
\item Minimizes page faults (primary goal)
\item Large page fault penalty (1 million cycles) justifies complexity
\item Different from cache (doesn't use tag comparators in memory)
\item Address translation through page table provides necessary mechanism

\subsection{Page Fault Handling}

\subsubsection{What Operating System Must Do Must Do}

\paragraph{1. Fetch Missing Page}

\begin{itemize}
\item Access disk to retrieve page
\item OS must know disk location of page
\item OS maintains data structures tracking page locations

\paragraph{2. Find Unused Frame}

\begin{itemize}
\item OS tracks which frames are currently used
\item Can determine this through page tables
\item If unused frame exists: Place page in unused frame

\paragraph{3. If Memory Full (No Unused Frames)}

\begin{itemize}
\item Select active page to replace using replacement policy
\item Common replacement policies:
\item Least Recently Used (LRU)
\item Pseudo-LRU (PLRU)
\item First-In-First-Out (FIFO)
\item Least Frequently Used (LFU)
\item Goal: Exploit temporal locality (keep recently/frequently used pages)

\paragraph{4. Check Dirty Bit of Page to be Replaced}

\begin{itemize}
\item If dirty bit = 1: Write page back to disk before replacement
\item If dirty bit = 0: Can directly overwrite (data consistent with disk)
\item Prevents data loss

\paragraph{5. Update Data Structures}

\begin{itemize}
\item Update page table entry for new page
\item Update page table entry for replaced page (set valid = 0)
\item Place fetched page in frame

\subsubsection{Optimization}

\begin{itemize}
\item Many operations can occur in parallel during disk fetch
\item While fetching data, OS can determine placement and handle replacement
\item Use buffers for write-back operations

\subsubsection{Why Software Handling?}

\begin{itemize}
\item 1 million cycle penalty is so large that software overhead is negligible
\item Complex replacement policies better suited to software
\item Hardware optimization doesn't provide significant benefit

\subsection{Translation Lookaside Buffer (TLB)}

![TLB](img/Virtual_mem_TLB.jpg)

\subsubsection{Purpose}

\begin{itemize}
\item Avoid accessing memory twice for every data access
\item Act as cache for page table entries
\item Reduce address translation overhead

\subsubsection{What is TLB?}

\begin{itemize}
\item Hardware cache specifically for page table entries
\item Stores recently used address translations
\item Based on locality of page table entry accesses
\item Exploits temporal and spatial locality of page accesses

\subsubsection{TLB Entry Structure}

\begin{itemize}
\item \textbf{Tag}: Virtual address tag (or physical address tag)
\item \textbf{Physical Page Number}: The address translation
\item \textbf{Valid Bit}: Is this TLB entry valid?
\item Different from page table valid bit
\item Indicates if TLB entry contains valid translation
\item \textbf{Dirty Bit}: Same meaning as in page table

\subsubsection{TLB Parametersrameters}

\paragraph{Size}

\begin{itemize}
\item \textbf{16-512 page table entries} (typical range)

\paragraph{Block Size}

\begin{itemize}
\item \textbf{1-2 address translations}
\item Small blocks because spatial locality between pages is larger
\item Adjacent pages not as closely related as adjacent cache blocks

\paragraph{Placement Policy}

\begin{itemize}
\item Fully associative or set associative
\item Fully associative for smaller TLBs (~16 entries)
\item Set associative for larger TLBs
\item Goal: Keep miss rate below 1%

\paragraph{Hit Latency}

\begin{itemize}
\item \textbf{Much less than 1 cycle}

\paragraph{Miss Penalty}

\begin{itemize}
\item \textbf{10-100 cycles} (memory access required)

\subsubsection{TLB Operationperation}

\paragraph{Hit}

\begin{itemize}
\item Address translation available in TLB
\item Use translation directly without accessing memory
\item Only one memory access needed (for data)

\paragraph{Miss}

\begin{itemize}
\item Translation not in TLB
\item Must access page table in memory
\item Total: Two memory accesses (page table + data)

\subsubsection{Why Low Miss Rate Essential?}

\begin{itemize}
\item TLB misses double memory access time
\item Must access page table (10-100 cycles) then data
\item Miss rate typically kept below 1%
\item > 99% of translations served by TLB

\subsection{Complete Memory Access with TLB}

Two different approaches for handling memory access with TLB:

\subsection{Approach 1: Virtually Addressed Cache}

\subsubsection{Process}

\begin{enumerate}
\item \textbf{CPU generates virtual address}
\item \textbf{Access cache with virtual address} (parallel with TLB)
\item \textbf{Cache Hit}: Return data to CPU immediately
\item \textbf{Cache Miss}:

   a. \textbf{Check TLB for address translation}

   b. \textbf{TLB Hit}:

\begin{itemize}
\item Get physical address
\item Access memory with physical address
\item Fetch missing block
\item Update cache
\item Send word to CPU

   c. \textbf{TLB Miss}:

\begin{itemize}
\item Access page table in memory
\item \textbf{Page Hit}:
\item Get translation
\item Access memory for data
\item Update TLB
\item Update cache
\item Send word to CPU
\item \textbf{Page Fault}:
\item OS accesses disk
\item Fetch missing page
\item Find unused frame or replace page
\item If replaced page dirty: write back
\item Update page table
\item Update TLB
\item Update cache
\item Send word to CPU

\subsubsection{Advantage}

\begin{itemize}
\item TLB access overlapped with cache access
\item Both happen in parallel
\item No additional latency for TLB access on cache hit

\subsection{Approach 2: Physically Addressed Cache}

\subsubsection{Process}

\begin{enumerate}
\item \textbf{CPU generates virtual address}
\item \textbf{Access TLB for translation first}
\item \textbf{TLB Hit}:

   a. Get physical address

   b. \textbf{Access cache with physical address}

   c. \textbf{Cache Hit}: Return data to CPU

   d. \textbf{Cache Miss}:

\begin{itemize}
\item Access memory with physical address
\item Fetch missing block
\item Update cache
\item Send word to CPU

\begin{enumerate}
\item \textbf{TLB Miss}:

   a. Access page table in memory

   b. \textbf{Page Hit}:

\begin{itemize}
\item Get translation
\item Update TLB
\item Access cache with physical address
\item If cache hit: return data
\item If cache miss: fetch from memory, update cache, return data

   c. \textbf{Page Fault}:

\begin{itemize}
\item OS handles as described above
\item Update page table, TLB, cache
\item Return data to CPU

\subsubsection{Advantage}

\begin{itemize}
\item Cache physically indexed and tagged
\item Simpler cache design
\item No aliasing issues

\subsubsection{Key Difference}

\begin{itemize}
\item \textbf{Approach 1}: Cache uses virtual addresses, TLB access parallel
\item \textbf{Approach 2}: Cache uses physical addresses, TLB access sequential

Both approaches are valid, and the choice depends on cache indexing method (virtual vs physical).

\subsection{Key Takeaways}

\begin{enumerate}
\item Virtual memory provides memory abstraction and protection
\item Address translation is fundamental to virtual memory operation
\item Page tables map virtual addresses to physical addresses
\item TLB caches translations to avoid double memory access
\item Page faults are extremely expensive (~1 million cycles)
\item Write-back policy is essential for virtual memory
\item Fully associative placement minimizes page faults
\item Multiple programs can safely share physical memory
\item OS handles page faults in software
\end{enumerate}

10. Virtual memory enables modern multitasking operating systems

\subsection{Summary}

Virtual memory represents a crucial abstraction in modern computing, enabling efficient and safe memory management across multiple concurrent programs while providing each program with the illusion of abundant, dedicated memory resources.
