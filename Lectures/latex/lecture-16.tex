\section{Lecture 16: Cache Write Policies and Associative Caches}

\emph{By Dr. Isuru Nawinne}

\subsection{Introduction}

This lecture explores advanced cache design techniques that significantly impact memory system performance. We examine write policies—specifically write-through and write-back strategies—understanding how each handles the critical challenge of maintaining consistency between cache and main memory while balancing performance and complexity. The lecture then progresses to associative cache organizations, from direct-mapped through set-associative to fully-associative designs, revealing how different levels of associativity affect hit rates, access latency, and hardware complexity. Through detailed examples and performance analysis, we discover how modern cache systems make strategic trade-offs between speed, capacity utilization, and implementation cost to achieve optimal memory hierarchy performance.

\subsection{Recap: Write Access in Direct Mapped Cache}

\subsubsection{Write-Through Policy}

\begin{itemize}
\item When a write access occurs, the cache controller determines if it's a hit or miss through tag comparison
\item On a write hit: The block is in cache, update it. The cache copy becomes different from memory (inconsistent)
\item Write-through solution: Always write to both cache and memory simultaneously
\item On a write miss: Stall the CPU, fetch the missing block from memory, update the cache, and write to memory

\subsubsection{Advantages of Write-Through}

\begin{itemize}
\item Simple to implement - straightforward cache controller design
\item Old blocks can be discarded without concern since memory is always up-to-date
\item Can overlap writing and tag comparison operations since corrupted data can be safely discarded on a miss

\subsubsection{Disadvantages of Write-Through}

\begin{itemize}
\item Generates heavy write traffic to memory
\item Every cache write triggers a memory write
\item Bus between cache and memory can become congested
\item Inefficient when programs have many store instructions
\item CPU must stall for 10-100 clock cycles on each memory write

\subsubsection{Write Buffer Solution}

\begin{itemize}
\item A FIFO (First In First Out) queue between cache and memory
\item Cache puts write requests in the buffer instead of directly to memory
\item Memory processes requests from buffer at its own speed
\item Allows CPU to continue without waiting for memory
\item Works well for burst writes (short sequences of writes with gaps between)
\item Limitation: If CPU generates continuous writes, buffer fills up and CPU must still stall

\subsection{Write-Back Policy}

\subsubsection{Basic Concept}

\begin{itemize}
\item Write to cache only, not to memory immediately
\item Allow cache and memory to be inconsistent
\item Write blocks back to memory only when evicted from cache

\subsubsection{Dirty Bit}

\begin{itemize}
\item An additional bit array in cache structure (alongside valid, tag, data)
\item Tracks whether a cache block has been modified
\item Set when block is written to cache
\item Indicates that memory copy is not up-to-date

\subsubsection{Write-Back Operations}

\textbf{On Write Hit:}

\begin{itemize}
\item Simply update the cache entry
\item Set the dirty bit to indicate inconsistency
\item Do not write to memory

\textbf{On Read Miss:}

\begin{itemize}
\item Fetch missing block from memory
\item If old block at that entry is dirty (dirty bit = 1):
\item Write old block back to memory first
\item Then fetch new block and overwrite
\item If old block is not dirty:
\item Directly fetch new block and overwrite

\textbf{On Write Miss:}

\begin{itemize}
\item If old block is dirty:
\item Write old block back to memory
\item Fetch new block from memory
\item Update cache entry only (not memory)

\subsubsection{Advantages of Write-Back}

\begin{itemize}
\item Significantly reduces write traffic to memory
\item More efficient when programs have many write accesses
\item Cache is fast; only writing to cache most of the time
\item Write buffer can be used for evicted dirty blocks

\subsubsection{Disadvantages of Write-Back}

\begin{itemize}
\item More complex cache controller
\item Need to maintain and check dirty bit
\item More hardware required
\item More logic in controller design

\subsubsection{Write-Back Cache Structure}

\begin{itemize}
\item Data array
\item Tag array
\item Valid bit array
\item Dirty bit array (new addition)

\subsection{Cache Performance}

\subsubsection{Average Access Time Formula}

T_avg = Hit Latency + Miss Rate $\times$ Miss Penalty

\textbf{Where:}

\begin{itemize}
\item Hit Latency: Time to determine a hit (always present)
\item Miss Rate: 1 - Hit Rate (fraction of accesses that are misses)
\item Miss Penalty: Time to fetch missing block from memory
\item Can be expressed in absolute time (nanoseconds) or clock cycles

\subsubsection{Example Calculation}

\textbf{Given:}

\begin{itemize}
\item Miss Penalty = 20 CPU cycles
\item Hit Rate = 95% (0.95)
\item Hit Latency = 1 CPU cycle
\item Clock Period = 1 nanosecond (1 GHz)

T_avg = 1 + (1 - 0.95) $\times$ 20 = 1 + 0.05 $\times$ 20 = 2 cycles = 2 nanoseconds

\textbf{If hit rate improves to 99.9\%:}

T_avg = 1 + (1 - 0.999) $\times$ 20 = 1 + 0.001 $\times$ 20 = 1.02 cycles

This shows significant improvement from better hit rate.

\subsubsection{Performance Example Problem}

\textbf{Given:}

\begin{itemize}
\item Program with 36% load/store instructions
\item Ideal CPI = 2 (assuming perfect caches)
\item Instruction cache miss rate = 2%
\item Data cache miss rate = 4%
\item Miss penalty = 100 cycles

\textbf{Calculating Actual CPI:}

\begin{itemize}
\item Stalls from instruction cache misses: I $\times$ 0.02 $\times$ 100 = 2I cycles
\item Stalls from data cache misses: I $\times$ 0.36 $\times$ 0.04 $\times$ 100 = 1.44I cycles
\item Total stall cycles: 3.44I
\item Actual CPI = 2 + 3.44 = 5.44 cycles per instruction

\textbf{Speedup with ideal caches:} 5.44 / 2 = 2.72$\times$

\textbf{CPI with no caches:}

\begin{itemize}
\item Every instruction fetch: 100 cycles
\item 36% need data memory: 0.36 $\times$ 100 = 36 cycles
\item Total CPI = 2 + 100 + 36 = 138 cycles
\item Slowdown without caches: 138 / 5.44 = 25.37$\times$

\subsection{Improving Cache Performance}

\subsubsection{Three Factors to Improve}

\begin{enumerate}
\item Hit Rate - increase the percentage of hits
\item Hit Latency - reduce time to determine hits
\item Miss Penalty - reduce time to fetch missing blocks

\subsubsection{Improving Hit Rate}

\textbf{Method 1: Larger Cache Size}

\begin{itemize}
\item More cache blocks means more index bits
\item Reduces probability of multiple addresses mapping to same index
\item Better exploitation of temporal locality
\item Trade-off: Higher cost (SRAM is expensive, ~$2000 per gigabyte)
\item Trade-off: More chip area required

\subsubsection{Direct Mapped Cache Limitation}

\begin{itemize}
\item Multiple memory blocks can map to same cache index
\item Even with empty cache blocks elsewhere, conflicts cause evictions
\item Temporal locality suggests recently accessed blocks should stay
\item But direct mapping forces eviction even when space is available

\subsection{Fully Associative Cache}

\subsubsection{Concept}

\begin{itemize}
\item Eliminate index field - no fixed mapping
\item A block can be placed anywhere in cache
\item Address divided into: Tag + Offset (no index)
\item Tag is larger since no index bits

\subsubsection{Finding Blocks}

\begin{itemize}
\item Cannot use index to locate block
\item Sequential search is too slow
\item Solution: Parallel tag comparison
\item Compare incoming tag with all stored tags simultaneously
\item Requires one comparator per cache entry

\subsubsection{Implementation}

\begin{itemize}
\item Need duplicate comparator hardware for each entry
\item Practical only for small number of entries (8, 16, 32, 64)
\item As entries increase: more comparators, longer wires, more delays

\subsubsection{Block Placement}

\begin{itemize}
\item Find first available invalid entry
\item Use sequential logic to search for invalid bit
\item Takes more time than direct mapped

\subsubsection{Block Replacement}

When all entries are valid, need replacement policy to choose which block to evict.

\subsubsection{Replacement Policies}

\begin{enumerate}
\item LRU (Least Recently Used) - IDEAL:

\begin{itemize}
\item Evict the block that was used longest ago
\item Best exploits temporal locality
\item Very complex to implement
\item Need to timestamp every access
\item Expensive in hardware

\begin{enumerate}
\item Pseudo-LRU (PLRU):

\begin{itemize}
\item Approximation of LRU
\item Simpler mechanism than true LRU
\item 90-99% of time picks least recently used
\item Better balance of performance and complexity

\begin{enumerate}
\item FIFO (First In First Out):
\item Evict block that entered cache first
\item Very simple implementation
\item Only update when new block fetched (not on every access)
\item Lower likelihood of picking LRU block
\item Used in embedded systems for simplicity and low power

\subsubsection{Fully Associative - Advantages}

\begin{itemize}
\item High utilization of cache space
\item Better hit rate (fewer conflict misses)
\item Can choose replacement policy based on needs

\subsubsection{Fully Associative - Disadvantages}

\begin{itemize}
\item Block placement is slow (increases miss penalty)
\item Higher power consumption
\item Higher cost (more hardware)
\item Parallel tag comparison requires duplicate hardware

\subsection{Set Associative Cache}

\subsubsection{Concept}

\begin{itemize}
\item Combines direct mapped and fully associative approaches
\item Add multiple "ways" - duplicate the tag/valid/data arrays
\item Each index refers to a "set" containing multiple blocks
\item Called "N-way set associative" where N is number of ways

\subsubsection{Two-Way Set Associative}

\begin{itemize}
\item Two copies of tag/valid/data arrays
\item Each index points to a set with 2 blocks
\item Index field selects the set
\item Tag comparison done in parallel within the set
\item Doubles cache capacity compared to direct mapped with same number of sets

\subsubsection{Read Access Process}

\begin{enumerate}
\item Use index to select correct set (via demultiplexer)
\item Extract both stored tags from the set
\item Parallel comparison of both tags with incoming tag
\item Each way has hit status (hit0, hit1)
\item Use encoder to generate select signal for multiplexer
\item Select correct data block based on which way hit
\item Use offset to select correct word within block

\subsubsection{Important Notes}

\begin{itemize}
\item Only one tag can match (each tag identifies unique block)
\item If no tags match, it's a miss
\item More complex hardware than direct mapped
\item Higher hit latency due to encoding and multiplexing delays

\subsection{Associativity Spectrum}

\subsubsection{For an 8-Block Cache, Different Organizations}

1-way set associative (Direct Mapped):

\begin{itemize}
\item 8 entries, 1 way each
\item 3-bit index
\item Each block has fixed location

2-way set associative:

\begin{itemize}
\item 4 entries, 2 ways each
\item 2-bit index
\item Each set can hold 2 different blocks

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{img/Memory%20Systems.jpg}
\caption{Memory System}
\end{figure}

4-way set associative:

\begin{itemize}
\item 2 entries, 4 ways each
\item 1-bit index
\item Each set can hold 4 different blocks

8-way set associative (Fully Associative):

\begin{itemize}
\item 1 entry, 8 ways
\item No index field (0 bits)
\item Any block can go anywhere

\subsubsection{Design Considerations}

\begin{itemize}
\item Choice depends on: program behavior, CPU architecture, performance goals, power budget
\item Higher associativity $\rightarrow$ better hit rate
\item Higher associativity $\rightarrow$ higher hit latency
\item Higher associativity $\rightarrow$ more power consumption and cost

\subsection{Associativity Comparison Example}

\subsubsection{Setup}

\begin{itemize}
\item Four-block cache (4 different blocks)
\item Block size = 1 word = 4 bytes
\item 8-bit addresses
\item Compare: Direct Mapped, 2-way Set Associative, Fully Associative (4-way)

\subsubsection{Initial State}

\begin{itemize}
\item All valid bits = 0 (invalid)
\item All tags = 0
\item Data unknown (don't care)

\subsubsection{Tag and Index Sizes}

\begin{itemize}
\item Direct Mapped: 4-bit tag, 2-bit index, 2-bit offset
\item 2-way Set Associative: 5-bit tag, 1-bit index, 2-bit offset
\item Fully Associative: 6-bit tag, 0-bit index, 2-bit offset

\subsubsection{Memory Access Sequence}

\textbf{Access 1: Block Address 0}

\begin{itemize}
\item All three caches: MISS (cold miss - first time accessed)
\item All valid bits were 0
\item Fetch from memory, update tag, set valid bit

\textbf{Score:} Direct Mapped: 0 hits, 1 miss | 2-way: 0 hits, 1 miss | Fully: 0 hits, 1 miss

\textbf{Access 2: Block Address 8}

\begin{itemize}
\item All three caches: MISS (cold miss - first time accessed)
\item Tags don't match existing entries
\item Fetch from memory, place in cache

\textbf{Score:} Direct Mapped: 0 hits, 2 misses | 2-way: 0 hits, 2 misses | Fully: 0 hits, 2 misses

\textbf{Access 3: Block Address 0 (repeated)}

\begin{itemize}
\item Direct Mapped: MISS (conflict miss - block 8 overwrote block 0 at same index)
\item 2-way Set Associative: HIT (both blocks 0 and 8 fit in same set)
\item Fully Associative: HIT (both blocks present)
\item Demonstrates advantage of associativity

\textbf{Score:} Direct Mapped: 0 hits, 3 misses | 2-way: 1 hit, 2 misses | Fully: 1 hit, 2 misses

\textbf{Access 4: Block Address 6}

\begin{itemize}
\item All three: MISS (cold miss)
\item 2-way: Set full, need replacement
\item LRU replacement: evict block 8 (least recently used)
\item FIFO replacement: would evict block 0 (first in)
\item Fully Associative: Still has empty space

\textbf{Score:} Direct Mapped: 0 hits, 4 misses | 2-way: 1 hit, 3 misses | Fully: 1 hit, 3 misses

\textbf{Access 5: Block Address 8 (repeated)}

\begin{itemize}
\item Direct Mapped: MISS (conflict miss - keeps conflicting at index 0)
\item 2-way: MISS (conflict miss - block 8 was evicted by block 6)
\item Fully Associative: HIT (block 8 still in cache)

\subsubsection{Final Score}

\begin{itemize}
\item Direct Mapped: 0 hits, 5 misses (all misses after cold misses)
\item 2-way Set Associative: 1 hit, 4 misses (one conflict miss)
\item Fully Associative: 2 hits, 3 misses (only cold misses)

\subsubsection{Types of Misses}

\begin{enumerate}
\item Cold Misses: First access to address (unavoidable)
\item Conflict Misses: Block evicted due to mapping, accessed again later

\subsubsection{Key Observations}

\begin{itemize}
\item Higher associativity reduces conflict misses
\item Fully associative eliminates conflict misses (only cold misses remain)
\item But higher associativity increases hit latency and cost

\subsection{Trade-Offs Summary}

\subsubsection{Hit Rate}

\begin{itemize}
\item Increases with higher associativity
\item Direct mapped has most conflict misses
\item Fully associative has only cold misses

\subsubsection{Hit Latency}

\begin{itemize}
\item Increases with higher associativity
\item More comparators, encoders, multiplexers add delay
\item Direct mapped is fastest

\subsubsection{Power and Cost}

\begin{itemize}
\item Increases with higher associativity
\item More hardware for parallel comparison
\item More complex control logic

\subsubsection{Design Decision Factors}

\begin{itemize}
\item Application requirements
\item Performance goals
\item Power budget
\item Cost constraints
\item Embedded systems often use lower associativity (FIFO replacement)
\item High-performance systems use higher associativity (PLRU replacement)

\subsection{Key Takeaways}

\begin{enumerate}
\item \textbf{Write policies} manage cache-memory consistency:
\item Write-through: Simple but generates heavy memory traffic
\item Write-back: More efficient but requires dirty bit tracking
\item \textbf{Write buffers} improve write-through performance by decoupling cache and memory writes
\item \textbf{Cache performance} depends on three factors: hit rate, hit latency, and miss penalty
\item \textbf{Associativity spectrum} ranges from direct-mapped (1-way) to fully associative (N-way)
\item \textbf{Higher associativity} reduces conflict misses and improves hit rate but increases complexity
\item \textbf{Set-associative caches} balance the trade-offs between direct-mapped and fully associative designs
\item \textbf{Replacement policies} (LRU, PLRU, FIFO) determine which block to evict in associative caches
\item \textbf{Design decisions} must balance performance, power consumption, cost, and complexity
\item \textbf{Real-world caches} use different associativity levels based on application requirements
\end{enumerate}

10. \textbf{Performance analysis} shows that even small improvements in hit rate significantly reduce average access time

\subsection{Summary}

This lecture examined two critical aspects of cache design: write policies and associativity. Write-through and write-back policies each offer distinct trade-offs between simplicity and efficiency, with write buffers providing a middle ground that improves performance without excessive complexity. The exploration of associative cache organizations revealed how different levels of associativity—from direct-mapped through set-associative to fully-associative—affect hit rates, access latency, and hardware complexity. Through detailed performance analysis and practical examples, we discovered that while higher associativity generally improves hit rates by reducing conflict misses, it comes at the cost of increased hit latency, power consumption, and implementation complexity. Modern cache systems carefully balance these competing factors, with set-associative designs emerging as an effective compromise that captures most of the benefits of full associativity while maintaining reasonable complexity. Understanding these design trade-offs is essential for optimizing memory hierarchy performance in real-world computer systems.
